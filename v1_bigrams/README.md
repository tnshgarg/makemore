# ğŸ§  Makemore â€” v1: Bigram Language Model

This is **Version 1** of my journey through Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* series.  
In this version, I implemented a **simple bigram character-level language model** from scratch using **Python + PyTorch**, trained on a dataset of human names.

---

## ğŸš€ What this model does

The goal is to **generate new, realistic-sounding names** by learning letter-to-letter transitions from a text file (`names.txt`).

At its core, the model learns:

> â€œGiven a current character, what is the probability of the next character?â€

This version doesnâ€™t use any deep learning yet â€” only a single **learnable matrix of weights** (`W`) that gets trained via **gradient descent**.  
But itâ€™s the perfect foundation to build intuition before we move on to neural networks (v2, v3, v4â€¦).
---

## ğŸ§© Concepts learned in v1

### 1. **Data Preparation**
- Loaded the dataset of names and tokenized it into individual characters.
- Added special start (`'.'` or `<S>`) and end (`'.'` or `<E>`) tokens.
- Created mappings:
  - `stoi` (string â†’ index)
  - `itos` (index â†’ string)

### 2. **Bigram Counting (Statistical Model)**
- Counted every letter pair (bigram) across all names.
- Built a **27Ã—27 matrix** `N` where `N[i,j]` = how many times letter *j* follows letter *i*.
- Normalized counts into probabilities `P` using Laplace smoothing.
- Sampled new names purely from this probability matrix.

### 3. **PyTorch Tensors & Autograd**
- Reimplemented the bigram model as a **learnable parameter matrix `W`**.
- Used **one-hot encoding** to represent input characters numerically.
- Computed probabilities with:
  ```
  probs = exp(logits) / sum(exp(logits))
  ```
- Defined a **negative log-likelihood loss** to measure how well the model predicts next letters.
- Used **autograd (`.backward()`)** and **manual gradient updates** to train `W`.

### 4. **Training**
- Ran multiple forward + backward passes to update weights.
- Observed loss decreasing (meaning the model is learning letter transitions).
- Generated names again â€” this time from a trained model instead of raw counts.

---

## ğŸ§® Technical Summary

| Component | Description |
|------------|-------------|
| **Input** | Previous character (one-hot encoded, 27-dim) |
| **Model** | Single weight matrix `W` (27Ã—27) |
| **Output** | Probability distribution for next character |
| **Loss** | Negative Log-Likelihood (Cross Entropy equivalent) |
| **Optimizer** | Manual SGD |
| **Dataset** | ~32,000 English names from `names.txt` |

---

## ğŸ“Š Sample Generated Output

After training, the model starts generating short, realistic name-like words:

```
morla.
kavon.
emala.
saril.
jodina.
```

These names arenâ€™t perfect, but they already reflect letter patterns found in real names â€” a huge step from random gibberish!

---

## ğŸ§  Key Takeaways

- Built first **generative model** from scratch.
- Learned how **probabilistic models** and **softmax** work.
- Understood **autograd** and **gradient descent** in practice.
- Gained intuition for what â€œtrainingâ€ really means before adding neural layers.

---

## ğŸ—‚ï¸ Project Structure

```
makemore/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ names.txt          # dataset of names
â”‚
â”œâ”€â”€ v1_bigrams/
â”‚   â”œâ”€â”€ makemore_v1.ipynb  # main notebook
â”‚   â”œâ”€â”€ README.md           # this file
â”‚   â””â”€â”€ output_samples.txt  # generated names (optional)
â”‚
â””â”€â”€ ...
```

---

## ğŸ§­ Next Steps (v2 Preview)

In **v2 (MLP)**, weâ€™ll:
- Move from one-letter context â†’ multi-letter context (more memory)
- Add hidden layers (non-linear transformations)
- Learn richer patterns for name generation

This transition marks the start of building a **true neural network**.

---

## ğŸ“š References
- [Andrej Karpathy â€“ Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [GitHub Repo: karpathy/nn-zero-to-hero](https://github.com/karpathy/nn-zero-to-hero)
