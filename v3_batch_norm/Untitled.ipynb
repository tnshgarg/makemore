{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b150b5d-3e93-4d80-bff3-d58679e1e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Three Building Blocks\n",
    "\n",
    "# 1. Linear Layer -> (like a calculator, takes 30 numbers, outputs 100 numbers)\n",
    "# 2. BatchNorm Layer -> (the balancer, as numbers pass through many layers, \n",
    "#    numbers can become too big or too small, BatchNorm keeps them reasonable)\n",
    "# 3. TanH Layer -> Takes any number and squishes it between 1 and -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137bfd1a-953f-4aae-8aa6-b373be9a10a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "words = open(\"../data/names.txt\", \"r\").read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8729772-c196-460b-905f-f34c29f6770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build character vocab\n",
    "chars = sorted(list(set(''.join(words))))  # all unique characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}  # string to index\n",
    "stoi['.'] = 0  # special token for start/end\n",
    "itos = {i:s for s,i in stoi.items()}  # index to string\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670d12a9-93a7-4d8f-b720-0d61b636ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c59e40c-9041-47c8-80f7-788096a801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Layer Class\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / (fan_in ** 0.5)\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Implement Forward Pass\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out = self.out + self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight, self.bias]\n",
    "        else:\n",
    "            return [self.weight]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0e90c2-cd8d-4ddd-8850-430be356798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 10])\n",
      "Output shape: torch.Size([3, 5])\n",
      "Number of parameters: 55\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "layer = Linear(10, 5)\n",
    "x = torch.randn(3, 10)  # 3 examples, 10 features\n",
    "out = layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")  # should be (3, 5)\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "120227e1-b11a-41d2-a650-07fc588a4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        \n",
    "        # Parameters (trainable)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "        # Buffers (not trainable)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Calculate forward pass\n",
    "        if self.training:\n",
    "            # Use batch statistics\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True, unbiased=False)\n",
    "        else:\n",
    "            # Use running statistics\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        # Normalize\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        \n",
    "        # Scale and shift\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        # Update running statistics\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # IMPORTANT: Remove the extra dimension from xmean and xvar\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.squeeze(0)\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar.squeeze(0)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af0dd189-4e40-4dac-81f4-c0557ffeef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before BatchNorm:\n",
      "Mean: tensor([49.0714, 49.1199, 44.7723, 51.2458, 48.2709])\n",
      "Std: tensor([12.1673, 10.4179, 11.7738, 11.9333, 14.3869])\n",
      "\n",
      "After BatchNorm:\n",
      "Mean: tensor([-4.8280e-07,  1.1921e-08, -2.2650e-07, -1.6689e-07,  1.2517e-07])\n",
      "Std: tensor([1.0541, 1.0541, 1.0541, 1.0541, 1.0541])\n",
      "\n",
      "Number of parameters: 10\n"
     ]
    }
   ],
   "source": [
    "# Test BatchNorm1d\n",
    "bn = BatchNorm1d(5)\n",
    "x = torch.randn(10, 5) * 10 + 50  # mean≈50, std≈10\n",
    "\n",
    "print(\"Before BatchNorm:\")\n",
    "print(f\"Mean: {x.mean(0)}\")\n",
    "print(f\"Std: {x.std(0)}\")\n",
    "\n",
    "out = bn(x)\n",
    "\n",
    "print(\"\\nAfter BatchNorm:\")\n",
    "print(f\"Mean: {out.mean(0)}\")  # should be close to 0\n",
    "print(f\"Std: {out.std(0)}\")    # should be close to 1\n",
    "\n",
    "print(f\"\\nNumber of parameters: {sum(p.nelement() for p in bn.parameters())}\")  # should be 10 (5 gamma + 5 beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df349c94-bf67-4af3-a272-8acbfd40b6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TanH Layer\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self,x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c43be43-657c-4ec4-a9f3-c11e10eb6935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([-5., -1.,  0.,  1.,  5.])\n",
      "Output: tensor([-0.9999, -0.7616,  0.0000,  0.7616,  0.9999])\n",
      "Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Test Tanh\n",
    "tanh = Tanh()\n",
    "x = torch.tensor([-5.0, -1.0, 0.0, 1.0, 5.0])\n",
    "out = tanh(x)\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {out}\")  # should be close to [-1, -0.76, 0, 0.76, 1]\n",
    "print(f\"Parameters: {len(tanh.parameters())}\")  # should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c44a7d4b-22ad-49a3-836f-ab142e7916b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Network Architecture\n",
    "n_embd = 10 # embedding dimension\n",
    "n_hidden = 100 #hidden layer size\n",
    "vocab_size = 27\n",
    "block_size = 3 # context length\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4babe006-c15f-4704-a2bd-2b5810097373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 10])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Character embedding table\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "999ddd26-4709-4875-9993-dc6607ac6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network layers\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c39c6f11-78fa-4a7b-bc96-2cdd0bff6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all parameters\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da70e642-4482-4a2d-9dd3-97dd29e79ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([32, 3, 10])\n",
      "Flattened shape: torch.Size([32, 30])\n",
      "Linear          -> torch.Size([32, 100])\n",
      "BatchNorm1d     -> torch.Size([32, 100])\n",
      "Tanh            -> torch.Size([32, 100])\n",
      "Linear          -> torch.Size([32, 100])\n",
      "BatchNorm1d     -> torch.Size([32, 100])\n",
      "Tanh            -> torch.Size([32, 100])\n",
      "Linear          -> torch.Size([32, 100])\n",
      "BatchNorm1d     -> torch.Size([32, 100])\n",
      "Tanh            -> torch.Size([32, 100])\n",
      "Linear          -> torch.Size([32, 100])\n",
      "BatchNorm1d     -> torch.Size([32, 100])\n",
      "Tanh            -> torch.Size([32, 100])\n",
      "Linear          -> torch.Size([32, 100])\n",
      "BatchNorm1d     -> torch.Size([32, 100])\n",
      "Tanh            -> torch.Size([32, 100])\n",
      "Linear          -> torch.Size([32, 27])\n",
      "BatchNorm1d     -> torch.Size([32, 27])\n",
      "\n",
      "Initial loss: 3.3173\n"
     ]
    }
   ],
   "source": [
    "# Testing forward pass\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)  # sample 32 examples\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "# Forward pass\n",
    "emb = C[Xb]  # (32, 3, 10) - 32 examples, 3 chars, 10 dimensions each\n",
    "print(f\"Embedding shape: {emb.shape}\")\n",
    "\n",
    "x = emb.view(emb.shape[0], -1)  # (32, 30) - flatten\n",
    "print(f\"Flattened shape: {x.shape}\")\n",
    "\n",
    "# Pass through all layers\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "    print(f\"{layer.__class__.__name__:15s} -> {x.shape}\")\n",
    "\n",
    "# Calculate loss\n",
    "loss = F.cross_entropy(x, Yb)\n",
    "print(f\"\\nInitial loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbf77a-18d7-4c26-a687-67056b3ace60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2661\n",
      "  10000/ 200000: 2.1920\n",
      "  20000/ 200000: 2.0275\n",
      "  30000/ 200000: 1.8103\n",
      "  40000/ 200000: 1.8109\n",
      "  50000/ 200000: 2.1085\n",
      "  60000/ 200000: 2.2533\n",
      "  70000/ 200000: 1.7763\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # 1. Simple Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # 2. Forward Pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1) # concatenate embeddings\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "    # 3. Backward Pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad()\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update parameters\n",
    "    lr = 0.1 if i < 150000 else 0.01\n",
    "\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # 5. Track statistics\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694b0ae-dd0d-45c6-92cc-71dc823cc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check activation health\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % \n",
    "              (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd1a98-e091-428b-a604-2690477cdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gradient health\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print('layer %d (%10s): mean %+f, std %e' % \n",
    "              (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "plt.legend(legends)\n",
    "plt.title('Gradient Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed21339-5251-4795-8b98-f1ba0e4d9a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check weight gradient distribution\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2:\n",
    "        print('weight %10s | mean %+f | std %e | grad:data ratio %e' % \n",
    "              (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('Weight Gradient Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13682176-d52d-4108-b2f6-35c18b734b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot update-to-data ratios\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k')  # target line at -3 (10^-3 = 0.001)\n",
    "plt.legend(legends)\n",
    "plt.title('Update to Data Ratio')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('log10(update/data ratio)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafdd03-2fd9-4c7e-b763-529ca4b883ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the model\n",
    "for layer in layers:\n",
    "    layer.training = False\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all '...'\n",
    "    \n",
    "    while True:\n",
    "        # Forward pass\n",
    "        emb = C[torch.tensor([context])]  # (1, block_size, n_embd)\n",
    "        x = emb.view(emb.shape[0], -1)  # concatenate\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        logits = x\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        \n",
    "        # Shift the context window and track samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        # If we sample '.', break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bd59b-fc42-4e79-9fbd-872d9f3d5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== EVAL MODE ==========\n",
    "for layer in layers:\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        layer.training = False\n",
    "\n",
    "# ========== BATCH SAMPLING (safer) ==========\n",
    "@torch.no_grad()\n",
    "def sample_names(num_samples=20):\n",
    "    \"\"\"Sample multiple names at once (batch inference)\"\"\"\n",
    "    g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        \n",
    "        while True:\n",
    "            # Create batch of size 32 (all same context)\n",
    "            contexts = torch.tensor([context] * 32)\n",
    "            \n",
    "            # Forward pass\n",
    "            emb = C[contexts]\n",
    "            x = emb.view(emb.shape[0], -1)\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "            \n",
    "            # Take only first prediction\n",
    "            logits = x[0:1]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Sample\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "            \n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "            \n",
    "            if ix == 0:\n",
    "                break\n",
    "        \n",
    "        print(''.join(itos[i] for i in out))\n",
    "\n",
    "sample_names(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba994a0-2d6f-4536-830c-7112c9ab6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking BatchNorm running statistics:\")\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, BatchNorm1d):\n",
    "        print(f\"\\nLayer {i}:\")\n",
    "        print(f\"  running_mean: {layer.running_mean[:5]}\")\n",
    "        print(f\"  running_var: {layer.running_var[:5]}\")\n",
    "        print(f\"  Has NaN? {torch.isnan(layer.running_mean).any() or torch.isnan(layer.running_var).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3608a1-431f-4d8b-b78e-235106153d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
